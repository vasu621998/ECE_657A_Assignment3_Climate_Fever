{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "maritime-democracy",
   "metadata": {},
   "source": [
    "# <div align=\"center\">CM1</div>\n",
    "\n",
    "\n",
    "###  1.1 Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim.downloader as api\n",
    "from datasets import load_dataset\n",
    "from scipy import spatial\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-hebrew",
   "metadata": {},
   "source": [
    "### 1.2 Load dataset \n",
    "\n",
    "- Identified there are large number of null values exist in columns, so in this assignment we have verified that there will be very less data exist if we remove null values.\n",
    "\n",
    "- Moreover we will focus on text dataset to build a model. Hence there is no good reason to normalise or remove nll values.\n",
    "\n",
    "\n",
    "**Dataset Overview :**\n",
    "\n",
    "- Climate Fever dataset uses Fever methodology that consists of 1,535 real-world claims regarding climate-change collected on the internet.\n",
    "\n",
    "- Each claim is accompanied by five manually annotated evidence ( Evidence 0 to 4 ) sentences retrieved from the  Wikipedia that support, refute or do not give enough information to validate the claim.\n",
    "\n",
    "**Dataset Feature :**\n",
    "\n",
    "1) claim_id : An unique claim identifier in datset.\n",
    "2) claim :  claim text.\n",
    "3) claim_label :   Overall label assigned to claim (based on evidence majority vote), The label correspond to 0: \"refutes\", 1: \"supports\" and 2: \"not enough info\".\n",
    "4) evidences : A list of evidences with below fields : \n",
    "    1. evidence_id : An unique evidence identifier.\n",
    "    2. evidence_label : A micro-verdict label, The label correspond to 0: \"refutes\", 1: \"supports\" and 2: \"not enough info\".\n",
    "    3. article : A title of source article (Wikipedia page).\n",
    "    4. evidence : An evidence sentence.\n",
    "    5. entropy : An entropy reflecting uncertainty of evidence_label.\n",
    "    6. votes : Refers to individual votes.\n",
    "    \n",
    "    \n",
    "**Note :** In each claim there are total 5 evidences avaialbke in dataset ( From evidence 0 to 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('climate_fever')\n",
    "dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-tennis",
   "metadata": {},
   "source": [
    "### 1.3 Assigned claim  and evidence feature values to corpus ( In order to to apply word embedding )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-latitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list = list()\n",
    "for i in range(0, dataset[\"test\"].num_rows):\n",
    "    claim = dataset[\"test\"][i][\"claim\"]\n",
    "    sent_list.append(claim)\n",
    "   \n",
    "    for _, data in enumerate(dataset[\"test\"][i][\"evidences\"]):\n",
    "        article = data[\"article\"]\n",
    "        evidence = data[\"evidence\"]\n",
    "        sent_list.append(article)\n",
    "        sent_list.append(evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-cabinet",
   "metadata": {},
   "source": [
    "### 1.4 Text/Data Preprocessing :\n",
    "\n",
    "- NLTK :\n",
    "    - We used nltk.corpus package and stopwords library to remove stopwords from corpus.\n",
    "- PorterStemmer :\n",
    "    - Stemming is the process of producing morphological variants of a root/base word.\n",
    "    - Stemming programs are commonly referred to as stemming algorithms or stemmers. A stemming algorithm reduces the words “chocolates”, “chocolatey”, “choco” to the root word, “chocolate”.\n",
    "    \n",
    "**Errors in Stemming :**\n",
    "There are mainly two errors in stemming – **Overstemming** and **Understemming**. \n",
    "- Overstemming occurs when two words are stemmed to same root that are of different stems. \n",
    "- Under-stemming occurs when two words are stemmed to same root that are not of different stems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "noticed-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "claim_corpus = []\n",
    "\n",
    "for i in range (0,1534):\n",
    "    sentences = re.sub('[^a-zA-z]',' ', corpus[i])\n",
    "    sentences = sentences.lower()\n",
    "    sentences = sentences.split()\n",
    "    ps = PorterStemmer()\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.remove('not')\n",
    "    sentences = [ps.stem(word) for word in sentences if not word in set(all_stopwords)]\n",
    "    sentences = ' '.join(sentences)\n",
    "    claim_corpus.append(sentences)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-transcription",
   "metadata": {},
   "source": [
    "### 1.5 Converted every word of the corpus to embedding vectors in order to embed the text dataset with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "muslim-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusVec = [nltk.word_tokenize(sentences) for sentences in claim_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-keeping",
   "metadata": {},
   "source": [
    "### 1.6 Build a word2Vec model\n",
    "\n",
    "- Word2Vec model maps words to real number vectors, at the same time capturing something about the meaning of the text. It says that if two words have similar meaning they will lie close to each other in the dense space. \n",
    "- Word2Vec model contains two models for training Skip-Gram model and continuous bag of words(CBOW).\n",
    "\n",
    "**Parameters :**\n",
    "- min_count (int) – Ignores all words with total frequency lower than min_count value\n",
    "- size (int) – Dimensionality of the feature vectors.\n",
    "- window (int) – The maximum distance between the current and predicted word within a sentence.\n",
    "- seed (int) – Seed for the random number generator. Initial vectors for each word are seeded with a hash of the concatenation of word + str(seed).\n",
    "\n",
    "**Note :**\n",
    "- We have build a Word2vec model on entire dataset and then after we have split word embeddings into train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "activated-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(corpusVec,  min_count =1)\n",
    "model.save(\"word2vec.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-expense",
   "metadata": {},
   "source": [
    "### 1.7 Split word embeddings into train and test sets\n",
    "\n",
    "- Train set (corpusVec_train) contains 80% of word embeddings.\n",
    "- Test set (corpusVec_test) contains 20% of word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fifth-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "corpusVec_train, corpusVec_test = train_test_split(corpusVec,test_size=0.20,random_state = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-inspection",
   "metadata": {},
   "source": [
    "#### 1.7.1 Created dictionaries to store word and it's embedding\n",
    "\n",
    "- 2 dictionaries are created ( 1 for train set and another for test set)\n",
    "- As a window_size in Word2Vec model is 100 (default value) for each word there will be 100 numbers attached to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "precise-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_train = dict()\n",
    "embedding_test = dict()\n",
    "\n",
    "\n",
    "for sentence in corpusVec_train:\n",
    "    for word in sentence:\n",
    "        embedding_train[word] = model.wv[word]\n",
    "        \n",
    "for sentence in corpusVec_test:\n",
    "    for word in sentence:\n",
    "        embedding_test[word] = model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "falling-fluid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2871"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bound-cloud",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1526"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-divorce",
   "metadata": {},
   "source": [
    "#### 1.7.2 Created train and test dataframe from embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "colonial-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb_df = pd.DataFrame.from_dict(embedding_train)\n",
    "test_emb_df = pd.DataFrame.from_dict(embedding_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "mighty-nepal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>earli</th>\n",
       "      <th>septemb</th>\n",
       "      <th>okeechobe</th>\n",
       "      <th>hurrican</th>\n",
       "      <th>made</th>\n",
       "      <th>landfal</th>\n",
       "      <th>near</th>\n",
       "      <th>west</th>\n",
       "      <th>palm</th>\n",
       "      <th>beach</th>\n",
       "      <th>...</th>\n",
       "      <th>till</th>\n",
       "      <th>coldest</th>\n",
       "      <th>themyscira</th>\n",
       "      <th>amazon</th>\n",
       "      <th>attack</th>\n",
       "      <th>creator</th>\n",
       "      <th>splice</th>\n",
       "      <th>sic</th>\n",
       "      <th>caet</th>\n",
       "      <th>unab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013419</td>\n",
       "      <td>0.008864</td>\n",
       "      <td>0.005450</td>\n",
       "      <td>0.049957</td>\n",
       "      <td>0.028665</td>\n",
       "      <td>0.003710</td>\n",
       "      <td>0.039027</td>\n",
       "      <td>0.032782</td>\n",
       "      <td>0.014662</td>\n",
       "      <td>0.006397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>-0.000966</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.009367</td>\n",
       "      <td>-0.000677</td>\n",
       "      <td>0.004301</td>\n",
       "      <td>-0.002030</td>\n",
       "      <td>0.001605</td>\n",
       "      <td>-0.000997</td>\n",
       "      <td>0.004693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013360</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>-0.107480</td>\n",
       "      <td>-0.062355</td>\n",
       "      <td>-0.013175</td>\n",
       "      <td>-0.075213</td>\n",
       "      <td>-0.064591</td>\n",
       "      <td>-0.027819</td>\n",
       "      <td>-0.007026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>-0.004554</td>\n",
       "      <td>-0.004754</td>\n",
       "      <td>-0.014114</td>\n",
       "      <td>-0.006255</td>\n",
       "      <td>-0.006049</td>\n",
       "      <td>-0.003033</td>\n",
       "      <td>-0.000787</td>\n",
       "      <td>-0.008450</td>\n",
       "      <td>0.000084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.023527</td>\n",
       "      <td>-0.008161</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>-0.058274</td>\n",
       "      <td>-0.032705</td>\n",
       "      <td>-0.007996</td>\n",
       "      <td>-0.038736</td>\n",
       "      <td>-0.040957</td>\n",
       "      <td>-0.019946</td>\n",
       "      <td>-0.001076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001400</td>\n",
       "      <td>-0.005210</td>\n",
       "      <td>-0.003454</td>\n",
       "      <td>-0.004284</td>\n",
       "      <td>-0.001268</td>\n",
       "      <td>-0.004208</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>-0.003174</td>\n",
       "      <td>0.001529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.025865</td>\n",
       "      <td>-0.016536</td>\n",
       "      <td>-0.000808</td>\n",
       "      <td>-0.089662</td>\n",
       "      <td>-0.048211</td>\n",
       "      <td>-0.009439</td>\n",
       "      <td>-0.064055</td>\n",
       "      <td>-0.053204</td>\n",
       "      <td>-0.026004</td>\n",
       "      <td>-0.004001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005228</td>\n",
       "      <td>-0.001169</td>\n",
       "      <td>-0.007759</td>\n",
       "      <td>-0.008618</td>\n",
       "      <td>-0.006645</td>\n",
       "      <td>-0.005677</td>\n",
       "      <td>-0.006545</td>\n",
       "      <td>-0.007614</td>\n",
       "      <td>-0.007933</td>\n",
       "      <td>-0.002934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.010680</td>\n",
       "      <td>-0.004536</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.046348</td>\n",
       "      <td>-0.032381</td>\n",
       "      <td>-0.002989</td>\n",
       "      <td>-0.033829</td>\n",
       "      <td>-0.025641</td>\n",
       "      <td>-0.010468</td>\n",
       "      <td>-0.001888</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005748</td>\n",
       "      <td>-0.004302</td>\n",
       "      <td>-0.000989</td>\n",
       "      <td>-0.011115</td>\n",
       "      <td>-0.006740</td>\n",
       "      <td>-0.004463</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.003083</td>\n",
       "      <td>-0.006569</td>\n",
       "      <td>-0.000615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2871 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      earli   septemb  okeechobe  hurrican      made   landfal      near  \\\n",
       "0  0.013419  0.008864   0.005450  0.049957  0.028665  0.003710  0.039027   \n",
       "1 -0.037344 -0.013360   0.000980 -0.107480 -0.062355 -0.013175 -0.075213   \n",
       "2 -0.023527 -0.008161   0.002991 -0.058274 -0.032705 -0.007996 -0.038736   \n",
       "3 -0.025865 -0.016536  -0.000808 -0.089662 -0.048211 -0.009439 -0.064055   \n",
       "4 -0.010680 -0.004536  -0.000056 -0.046348 -0.032381 -0.002989 -0.033829   \n",
       "\n",
       "       west      palm     beach  ...      till   coldest  themyscira  \\\n",
       "0  0.032782  0.014662  0.006397  ...  0.001144 -0.000966    0.002964   \n",
       "1 -0.064591 -0.027819 -0.007026  ...  0.000229 -0.004554   -0.004754   \n",
       "2 -0.040957 -0.019946 -0.001076  ... -0.001400 -0.005210   -0.003454   \n",
       "3 -0.053204 -0.026004 -0.004001  ... -0.005228 -0.001169   -0.007759   \n",
       "4 -0.025641 -0.010468 -0.001888  ... -0.005748 -0.004302   -0.000989   \n",
       "\n",
       "     amazon    attack   creator    splice       sic      caet      unab  \n",
       "0  0.009367 -0.000677  0.004301 -0.002030  0.001605 -0.000997  0.004693  \n",
       "1 -0.014114 -0.006255 -0.006049 -0.003033 -0.000787 -0.008450  0.000084  \n",
       "2 -0.004284 -0.001268 -0.004208  0.001467  0.002422 -0.003174  0.001529  \n",
       "3 -0.008618 -0.006645 -0.005677 -0.006545 -0.007614 -0.007933 -0.002934  \n",
       "4 -0.011115 -0.006740 -0.004463  0.000131  0.003083 -0.006569 -0.000615  \n",
       "\n",
       "[5 rows x 2871 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_emb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "electoral-medicaid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=3232, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-miracle",
   "metadata": {},
   "source": [
    "#### 1.7.3 Features of Word2Vec :\n",
    "\n",
    "- After the model is trained, it is accessible via the “wv” attribute..This is the actual word vector model in which queries can be made.\n",
    "- We can also save and load out Word2Vec model using \"model.save(\"word2vec.bin\")\" and \"model = Word2Vec.load('model.bin')\" respectively.\n",
    "- We can print the learned vocabulary of tokens (words) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aggregate-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = model.wv.vocab\n",
    "# print(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-order",
   "metadata": {},
   "source": [
    "### 1.8 Cosine Similarity Function ( In order to calulate text similarity ): \n",
    "\n",
    "- Among different distance metrics, cosine similarity is more intuitive and most used in word2vec. It is normalized dot product of 2 vectors and this ratio defines the angle between them.\n",
    "- Cosine similiarity between two vectors (A and B) can be calculated using dot(A, B)/(norm(A)*norm(B)). Here norm(A) and norm(B) indicates euclidean norm of vectors respectively.\n",
    "\n",
    "**Note :** In order to build a function for cosine similarity we used scipy.spatial.distance.cosine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "extra-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(v1, v2):\n",
    "    return abs(1 - spatial.distance.cosine(v1,v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-penetration",
   "metadata": {},
   "source": [
    "#### 1.8.1 Cosine Similarity Function Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "novel-midnight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between 'slower'and 'global' is : 0.9800435900688171\n",
      "Cosine Similarity between 'high'and 'low' is : 0.9986007809638977\n",
      "Cosine Similarity between 'peopl'and 'human' is : 0.9858113527297974\n",
      "Cosine Similarity between 'australia'and 'warm' is : 0.9992226958274841\n",
      "Cosine Similarity between 'scientists'and 'guidelin' is : 0.47627413272857666\n"
     ]
    }
   ],
   "source": [
    "ex1 = cos_similarity(embedding_train[\"slower\"], embedding_train[\"global\"])\n",
    "ex2 = cos_similarity(embedding_train[\"high\"], embedding_train[\"low\"])\n",
    "ex3 = cos_similarity(embedding_train[\"peopl\"], embedding_train[\"human\"])\n",
    "ex4 = cos_similarity(embedding_train[\"australia\"], embedding_train[\"warm\"])\n",
    "ex5 = cos_similarity(embedding_train[\"scientists\"], embedding_train[\"guidelin\"])\n",
    "print(f\"Cosine Similarity between 'slower'and 'global' is : {ex1}\")\n",
    "print(f\"Cosine Similarity between 'high'and 'low' is : {ex2}\")\n",
    "print(f\"Cosine Similarity between 'peopl'and 'human' is : {ex3}\")\n",
    "print(f\"Cosine Similarity between 'australia'and 'warm' is : {ex4}\")\n",
    "print(f\"Cosine Similarity between 'scientists'and 'guidelin' is : {ex5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-custom",
   "metadata": {},
   "source": [
    "#### 1.8.2 Analysis of Cosine Similarity \n",
    "\n",
    "- Cosine Similarity measured by the cosine of **the angle between two vectors** and determines whether two vectors are pointing in roughly the same direction or not.\n",
    "- This similarity score ranges from 0 to 1, with 0 being the lowest (the least similar) and 1 being the highest (the most similar).\n",
    "-  A cosine value of 0 means that the two vectors are at 90 degrees to each other (orthogonal) and have no match. \n",
    "- The closer the cosine value to 1, the smaller the angle and the greater the match between vectors.\n",
    "- From above examples and embedded vector for a specific token ( model.wv[word] ) it is cleared that **more difference** between embedded vector for a specific token then lower value for cosine simillarity. ( for example scientists and guidelin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-crime",
   "metadata": {},
   "source": [
    "### 1.9 Airthmetic computations on embedding vectors\n",
    "\n",
    "- After successfully build a Word2Vec model, we can do create a little linear algebra arithmetic with words.\n",
    "- Gensim provides an interface for performing these types of operations in the most_similar() function on the trained or loaded model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-memorial",
   "metadata": {},
   "source": [
    "Example 1 : **ocean - water + arctic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "mineral-snake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('anthropogen', 0.9971425533294678),\n",
       " ('chang', 0.9971103072166443),\n",
       " ('per', 0.9970500469207764),\n",
       " ('like', 0.9970331788063049),\n",
       " ('also', 0.9970329999923706),\n",
       " ('year', 0.9970141649246216),\n",
       " ('result', 0.9969948530197144),\n",
       " ('report', 0.996990442276001),\n",
       " ('global', 0.9969826936721802),\n",
       " ('sinc', 0.9969472885131836)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['ocean', 'arctic'], negative=['water'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-account",
   "metadata": {},
   "source": [
    "Example 2 :  **carbon - dioxide + oxygen** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "systematic-column",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('train', 0.5851308703422546),\n",
       " ('neutral', 0.5764247179031372),\n",
       " ('heart', 0.5706777572631836),\n",
       " ('usd', 0.5703952312469482),\n",
       " ('corp', 0.5674421191215515),\n",
       " ('microbi', 0.565788984298706),\n",
       " ('context', 0.5622187852859497),\n",
       " ('threaten', 0.5617069005966187),\n",
       " ('noth', 0.5592998266220093),\n",
       " ('guidelin', 0.5588611364364624)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['carbon', 'oxygen'], negative=['dioxide'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-prayer",
   "metadata": {},
   "source": [
    "Example 3: **'heat' - 'rise' + 'cold'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "written-earth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model', 0.9943777322769165),\n",
       " ('food', 0.9943655729293823),\n",
       " ('recent', 0.994345486164093),\n",
       " ('continu', 0.9943419694900513),\n",
       " ('weather', 0.9942892789840698),\n",
       " ('research', 0.994236946105957),\n",
       " ('high', 0.9942196607589722),\n",
       " ('scientist', 0.994118332862854),\n",
       " ('rate', 0.9941150546073914),\n",
       " ('report', 0.9941065311431885)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['heat', 'cold', 'warm'], negative=['rise'])\n",
    "#model.wv.most_similar('heat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-chile",
   "metadata": {},
   "source": [
    "Example 4 : **'high' - 'temperatur' + 'heat'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "subjective-bracket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('event', 0.9989520907402039),\n",
       " ('weather', 0.9989404082298279),\n",
       " ('record', 0.9988930225372314),\n",
       " ('water', 0.9988844990730286),\n",
       " ('climat', 0.9988824129104614),\n",
       " ('chang', 0.9988715648651123),\n",
       " ('report', 0.998858630657196),\n",
       " ('use', 0.998855710029602),\n",
       " ('cycl', 0.998854398727417),\n",
       " ('world', 0.9988488554954529)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['high', 'heat'], negative=['temperatur'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-appreciation",
   "metadata": {},
   "source": [
    "Example 4 : **'climat' + 'chang' + 'temperatur'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['climat', 'chang', 'temperatur'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-baseball",
   "metadata": {},
   "source": [
    "Example 5 : **'hurrican' - 'storm' + 'wind'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['hurrican', 'wind'], negative=['storm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dressed-exercise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99860054\n",
      "0.9858114\n",
      "0.99922276\n",
      "0.5082087\n"
     ]
    }
   ],
   "source": [
    "### Examples : \n",
    "print(model.wv.similarity('high', 'low'))\n",
    "print(model.wv.similarity('peopl', 'human'))\n",
    "# model.wv.similarity('upstream', 'downstream')\n",
    "print(model.wv.similarity('australia', 'warm'))\n",
    "print(model.wv.similarity('shrink', 'compress'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "angry-sunglasses",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996577"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('heat', 'water')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-visibility",
   "metadata": {},
   "source": [
    "### 1.10 Loaded pretrained model and performed \n",
    "\n",
    "We have loaded below to pretrained model :\n",
    "\n",
    "1. glove-wiki-gigaword-50 model\n",
    "2. GoogleNewsvectorsnegative300 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "funded-columbus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-50\")\n",
    "#print(api.load('glove-wiki-gigaword-50', return_path=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "warming-anger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('temperature', 0.7556608319282532),\n",
       " ('cold', 0.7549160718917847),\n",
       " ('temperatures', 0.7502944469451904),\n",
       " ('humidity', 0.7431684732437134),\n",
       " ('hot', 0.7388110160827637),\n",
       " ('moisture', 0.7275059819221497),\n",
       " ('chill', 0.7268701195716858),\n",
       " ('water', 0.7164075970649719),\n",
       " ('heating', 0.7090611457824707),\n",
       " ('add', 0.7034947872161865)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_vectors.wv.most_similar(positive=['heat', 'volcano'], negative=['ice'])\n",
    "\n",
    "word_vectors.most_similar('heat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-harbor",
   "metadata": {},
   "source": [
    "**Example 1 in Pretrained model :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "white-restriction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-49-2f7f1a1dd2a2>:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  X1 = model[model.wv.vocab]\n"
     ]
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['ocean', 'arctic'], negative=['water'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-recording",
   "metadata": {},
   "source": [
    "**Example 2 in Pretrained model :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=['carbon', 'oxygen'], negative=['dioxide'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-vertical",
   "metadata": {},
   "source": [
    "**Example 3 in Pretrained model :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=['heat', 'cold', 'warm'], negative=['rise'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-calculation",
   "metadata": {},
   "source": [
    "**Example 4 in Pretrained model :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=['climat', 'chang', 'temperatur'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-offering",
   "metadata": {},
   "source": [
    "**Example 5 in Pretrained model :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=['hurrican', 'wind'], negative=['storm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "seventh-accountability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "import gensim.downloader as api\n",
    "#word_vectors2 = api.load(\"GoogleNews-vectors-negative300.bin.gz\")\n",
    "\n",
    "pr_model2 = api.load(\"glove-twitter-25\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "helpful-eating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thunder', 0.9267944097518921),\n",
       " ('bulls', 0.911600649356842),\n",
       " ('ball', 0.9085021615028381),\n",
       " ('beat', 0.9076730012893677),\n",
       " ('playoffs', 0.8956629037857056),\n",
       " ('lakers', 0.8936164379119873),\n",
       " ('basketball', 0.8829619884490967),\n",
       " ('cowboys', 0.8791283965110779),\n",
       " ('baseball', 0.8777887225151062),\n",
       " ('celtics', 0.8775449991226196)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_model2.most_similar('heat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "elementary-consumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "# load the Stanford GloVe model\n",
    "#filename = 'GoogleNews-vectors-negative300.bin'\n",
    "#trained_model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "m1 = gensim.models.KeyedVectors.load_word2vec_format('GoogleNewsvectorsnegative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.most_similar('heat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "water = model.wv[\"year\"]\n",
    "ice = model.wv[\"ice\"]\n",
    "similarity = cosine_similarity(water, ice)\n",
    "print(f\"The Cosine Similarity between 'water' and 'ice': {similarity:}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-cradle",
   "metadata": {},
   "source": [
    "### Cosine Similarity Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity(embedding_train[\"slower\"], embedding_train[\"global\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('slower', 'global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-right",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
